{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Volume Forecasting\n",
    "\n",
    "This notebook demonstrates the feature engineering pipeline for the volume forecasting project. We will:\n",
    "\n",
    "1. Load synthetic volume data\n",
    "2. Apply the `FeaturePipeline` to create all features\n",
    "3. Explore and visualize the generated features\n",
    "4. Analyze feature correlations with the target variable\n",
    "5. Discuss feature importance for modeling\n",
    "\n",
    "## Feature Categories\n",
    "\n",
    "The pipeline creates three main categories of features:\n",
    "\n",
    "- **Temporal Features**: Day of week, month, weekend flags, cyclical encodings\n",
    "- **Lag Features**: Previous day values at various lags (1, 7, 14, 21 days)\n",
    "- **Rolling Features**: Moving averages and standard deviations (7, 14, 30 day windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from volume_forecast.features import FeaturePipeline\n",
    "from volume_forecast.data_generation import VolumeGenerator\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Synthetic Volume Data\n",
    "\n",
    "First, we load the synthetic volume data. If the file does not exist, we generate it using the `VolumeGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_path = project_root / \"data\" / \"raw\" / \"synthetic_volumes.csv\"\n",
    "\n",
    "# Generate data if it doesn't exist\n",
    "if not data_path.exists():\n",
    "    print(\"Generating synthetic volume data...\")\n",
    "    generator = VolumeGenerator(seed=42)\n",
    "    df = generator.generate(\n",
    "        start_date=date(2023, 1, 1),\n",
    "        end_date=date(2024, 12, 31),\n",
    "        include_events=True\n",
    "    )\n",
    "    generator.save(df, data_path)\n",
    "    print(f\"Saved {len(df)} rows to {data_path}\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path, parse_dates=['date'])\n",
    "    print(f\"Loaded {len(df)} rows from {data_path}\")\n",
    "\n",
    "# Ensure date column is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of raw data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply the Feature Pipeline\n",
    "\n",
    "The `FeaturePipeline` combines three transformers:\n",
    "\n",
    "1. **TemporalFeatures**: Extracts calendar-based features from the date column\n",
    "2. **LagFeatures**: Creates lagged versions of target columns\n",
    "3. **RollingFeatures**: Computes rolling window statistics\n",
    "\n",
    "Let's apply the pipeline to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature pipeline\n",
    "pipeline = FeaturePipeline(\n",
    "    date_column='date',\n",
    "    target_columns=['daily_logins', 'daily_deposits'],\n",
    "    lags=[1, 7, 14, 21],\n",
    "    rolling_windows=[7, 14, 30],\n",
    "    rolling_stats=['mean', 'std'],\n",
    "    cyclical=True\n",
    ")\n",
    "\n",
    "# Display pipeline configuration\n",
    "print(\"Pipeline Parameters:\")\n",
    "for key, value in pipeline.get_params().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n",
    "pipeline.fit(df)\n",
    "df_features = pipeline.transform(df)\n",
    "\n",
    "print(f\"Original columns: {len(df.columns)}\")\n",
    "print(f\"After feature engineering: {len(df_features.columns)}\")\n",
    "print(f\"New features added: {len(df_features.columns) - len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full List of Generated Features\n",
    "\n",
    "Let's examine all the features created by the pipeline, organized by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from pipeline\n",
    "feature_names = pipeline.get_feature_names()\n",
    "\n",
    "print(f\"Total features generated: {len(feature_names)}\\n\")\n",
    "\n",
    "# Categorize features\n",
    "temporal_features = [f for f in feature_names if not any(x in f for x in ['lag_', 'rolling_'])]\n",
    "lag_features = [f for f in feature_names if 'lag_' in f]\n",
    "rolling_features = [f for f in feature_names if 'rolling_' in f]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "for f in temporal_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"LAG FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "for f in lag_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"ROLLING FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "for f in rolling_features:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transformed data\n",
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (expected in lag/rolling features at the start)\n",
    "missing = df_features.isnull().sum()\n",
    "missing_cols = missing[missing > 0]\n",
    "\n",
    "print(\"Columns with missing values (due to lag/rolling windows):\")\n",
    "print(missing_cols.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Heatmap: Features vs Target\n",
    "\n",
    "Let's visualize how each feature correlates with our target variables (`daily_logins` and `daily_deposits`). This helps identify the most predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for correlation analysis (drop rows with NaN)\n",
    "df_clean = df_features.dropna()\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_clean[numeric_cols].corr()\n",
    "\n",
    "print(f\"Shape after dropping NaN rows: {df_clean.shape}\")\n",
    "print(f\"Numeric columns for correlation: {len(numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract correlations with target variables\n",
    "targets = ['daily_logins', 'daily_deposits']\n",
    "corr_with_targets = corr_matrix[targets].drop(targets)\n",
    "\n",
    "# Sort by correlation with daily_logins\n",
    "corr_sorted = corr_with_targets.sort_values('daily_logins', ascending=False)\n",
    "\n",
    "# Plot correlation heatmap for targets\n",
    "fig, ax = plt.subplots(figsize=(10, 14))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_sorted,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Correlation'}\n",
    ")\n",
    "\n",
    "ax.set_title('Feature Correlations with Target Variables', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Target Variables')\n",
    "ax.set_ylabel('Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 features by absolute correlation with daily_logins\n",
    "top_corr = corr_with_targets['daily_logins'].abs().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Features by Correlation with daily_logins:\")\n",
    "print(\"=\" * 50)\n",
    "for feat, corr in top_corr.items():\n",
    "    actual_corr = corr_with_targets.loc[feat, 'daily_logins']\n",
    "    print(f\"{feat:40} {actual_corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lag Feature Relationships\n",
    "\n",
    "Lag features capture the autocorrelation structure of time series. Let's visualize how the current value relates to its past values at different lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag feature scatter plots for daily_logins\n",
    "lags = [1, 7, 14, 21]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, lag in enumerate(lags):\n",
    "    ax = axes[idx]\n",
    "    lag_col = f'daily_logins_lag_{lag}'\n",
    "    \n",
    "    # Remove NaN for this lag\n",
    "    mask = df_features[lag_col].notna()\n",
    "    x = df_features.loc[mask, lag_col]\n",
    "    y = df_features.loc[mask, 'daily_logins']\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(x, y, alpha=0.3, s=20)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(x.min(), x.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r-', linewidth=2, label='Trend')\n",
    "    \n",
    "    # Add diagonal reference\n",
    "    min_val = min(x.min(), y.min())\n",
    "    max_val = max(x.max(), y.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='y=x')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = np.corrcoef(x, y)[0, 1]\n",
    "    \n",
    "    ax.set_xlabel(f'daily_logins (t-{lag})')\n",
    "    ax.set_ylabel('daily_logins (t)')\n",
    "    ax.set_title(f'Lag {lag} Days (r={corr:.3f})', fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "plt.suptitle('Lag Feature Relationships for daily_logins', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation function comparison\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, col in enumerate(['daily_logins', 'daily_deposits']):\n",
    "    ax = axes[idx]\n",
    "    autocorrelation_plot(df_features[col].dropna(), ax=ax)\n",
    "    ax.set_title(f'Autocorrelation: {col}', fontweight='bold')\n",
    "    ax.set_xlim(0, 60)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axvline(x=7, color='r', linestyle='--', alpha=0.7, label='Weekly')\n",
    "    ax.axvline(x=14, color='g', linestyle='--', alpha=0.7, label='Bi-weekly')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Autocorrelation Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rolling Statistics Over Time\n",
    "\n",
    "Rolling features smooth out noise and capture trends. Let's visualize how the rolling mean and standard deviation evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling mean comparison for daily_logins\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot rolling means\n",
    "ax = axes[0]\n",
    "ax.plot(df_features['date'], df_features['daily_logins'], \n",
    "        alpha=0.4, label='Actual', color='gray', linewidth=0.8)\n",
    "ax.plot(df_features['date'], df_features['daily_logins_rolling_mean_7'], \n",
    "        label='7-day MA', linewidth=1.5)\n",
    "ax.plot(df_features['date'], df_features['daily_logins_rolling_mean_14'], \n",
    "        label='14-day MA', linewidth=1.5)\n",
    "ax.plot(df_features['date'], df_features['daily_logins_rolling_mean_30'], \n",
    "        label='30-day MA', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Daily Logins')\n",
    "ax.set_title('Rolling Mean Features for daily_logins', fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot rolling standard deviations\n",
    "ax = axes[1]\n",
    "ax.plot(df_features['date'], df_features['daily_logins_rolling_std_7'], \n",
    "        label='7-day Std', linewidth=1.5)\n",
    "ax.plot(df_features['date'], df_features['daily_logins_rolling_std_14'], \n",
    "        label='14-day Std', linewidth=1.5)\n",
    "ax.plot(df_features['date'], df_features['daily_logins_rolling_std_30'], \n",
    "        label='30-day Std', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Standard Deviation')\n",
    "ax.set_title('Rolling Standard Deviation Features for daily_logins', fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling statistics distribution by window size\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "windows = [7, 14, 30]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for idx, window in enumerate(windows):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    mean_col = f'daily_logins_rolling_mean_{window}'\n",
    "    std_col = f'daily_logins_rolling_std_{window}'\n",
    "    \n",
    "    # Create scatter plot of mean vs std\n",
    "    ax.scatter(\n",
    "        df_features[mean_col].dropna(),\n",
    "        df_features[std_col].dropna(),\n",
    "        alpha=0.4,\n",
    "        s=20,\n",
    "        color=colors[idx]\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel(f'{window}-day Rolling Mean')\n",
    "    ax.set_ylabel(f'{window}-day Rolling Std')\n",
    "    ax.set_title(f'{window}-day Window', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Rolling Mean vs Standard Deviation', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cyclical Feature Patterns (Sin/Cos)\n",
    "\n",
    "Cyclical features encode periodic patterns (day of week, month, week of year) using sine and cosine transformations. This ensures that:\n",
    "- Monday (0) is close to Sunday (6) in the feature space\n",
    "- December (12) is close to January (1)\n",
    "\n",
    "This is more appropriate for ML models than raw integer encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cyclical encoding for day of week\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Day of week: sin vs cos\n",
    "ax = axes[0, 0]\n",
    "scatter = ax.scatter(\n",
    "    df_features['day_of_week_cos'],\n",
    "    df_features['day_of_week_sin'],\n",
    "    c=df_features['day_of_week'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=20\n",
    ")\n",
    "plt.colorbar(scatter, ax=ax, label='Day of Week (0=Mon, 6=Sun)')\n",
    "ax.set_xlabel('cos(day_of_week)')\n",
    "ax.set_ylabel('sin(day_of_week)')\n",
    "ax.set_title('Day of Week Cyclical Encoding', fontweight='bold')\n",
    "ax.set_xlim(-1.2, 1.2)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add day labels\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for day in range(7):\n",
    "    angle = 2 * np.pi * day / 7\n",
    "    x, y = np.cos(angle), np.sin(angle)\n",
    "    ax.annotate(day_names[day], (x*1.1, y*1.1), ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Month: sin vs cos\n",
    "ax = axes[0, 1]\n",
    "# Sample for clarity\n",
    "sample = df_features.drop_duplicates(subset=['month'])\n",
    "scatter = ax.scatter(\n",
    "    df_features['month_cos'],\n",
    "    df_features['month_sin'],\n",
    "    c=df_features['month'],\n",
    "    cmap='hsv',\n",
    "    alpha=0.6,\n",
    "    s=20\n",
    ")\n",
    "plt.colorbar(scatter, ax=ax, label='Month (1-12)')\n",
    "ax.set_xlabel('cos(month)')\n",
    "ax.set_ylabel('sin(month)')\n",
    "ax.set_title('Month Cyclical Encoding', fontweight='bold')\n",
    "ax.set_xlim(-1.2, 1.2)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Week: sin vs cos\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(\n",
    "    df_features['week_cos'],\n",
    "    df_features['week_sin'],\n",
    "    c=df_features['week_of_year'],\n",
    "    cmap='twilight',\n",
    "    alpha=0.6,\n",
    "    s=20\n",
    ")\n",
    "plt.colorbar(scatter, ax=ax, label='Week of Year (1-52)')\n",
    "ax.set_xlabel('cos(week)')\n",
    "ax.set_ylabel('sin(week)')\n",
    "ax.set_title('Week of Year Cyclical Encoding', fontweight='bold')\n",
    "ax.set_xlim(-1.2, 1.2)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Target vs cyclical features\n",
    "ax = axes[1, 1]\n",
    "daily_by_dow = df_features.groupby('day_of_week')['daily_logins'].mean()\n",
    "bars = ax.bar(day_names, daily_by_dow.values, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Day of Week')\n",
    "ax.set_ylabel('Average Daily Logins')\n",
    "ax.set_title('Average Logins by Day of Week', fontweight='bold')\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly patterns visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Monthly averages\n",
    "monthly_avg = df_features.groupby('month')[['daily_logins', 'daily_deposits']].mean()\n",
    "\n",
    "ax = axes[0]\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "x = np.arange(len(months))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, monthly_avg['daily_logins'], width, label='Logins', alpha=0.8)\n",
    "ax.bar(x + width/2, monthly_avg['daily_deposits'], width, label='Deposits', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(months)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Average Daily Count')\n",
    "ax.set_title('Seasonal Patterns by Month', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Weekend effect\n",
    "ax = axes[1]\n",
    "weekend_avg = df_features.groupby('is_weekend')[['daily_logins', 'daily_deposits']].mean()\n",
    "\n",
    "x = ['Weekday', 'Weekend']\n",
    "width = 0.35\n",
    "x_pos = np.arange(len(x))\n",
    "\n",
    "ax.bar(x_pos - width/2, weekend_avg['daily_logins'], width, label='Logins', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, weekend_avg['daily_deposits'], width, label='Deposits', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x)\n",
    "ax.set_xlabel('Day Type')\n",
    "ax.set_ylabel('Average Daily Count')\n",
    "ax.set_title('Weekend vs Weekday Effect', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Discussion\n",
    "\n",
    "Based on our analysis, here are the key insights about feature importance for modeling:\n",
    "\n",
    "### High-Impact Features\n",
    "\n",
    "1. **Lag Features (especially lag_7)**\n",
    "   - Strong weekly autocorrelation (~0.7-0.8) in both logins and deposits\n",
    "   - Same-day-last-week is typically the strongest predictor\n",
    "   - Captures weekly seasonality effectively\n",
    "\n",
    "2. **Rolling Mean Features**\n",
    "   - Smooth out daily noise\n",
    "   - 7-day and 14-day windows capture recent trends\n",
    "   - 30-day window captures monthly baseline\n",
    "\n",
    "3. **Day of Week Features**\n",
    "   - Clear weekly pattern: Saturday peak, Tuesday trough\n",
    "   - Cyclical encoding (sin/cos) preferred for tree-based models\n",
    "   - One-hot encoding may work better for linear models\n",
    "\n",
    "### Medium-Impact Features\n",
    "\n",
    "4. **Month/Seasonal Features**\n",
    "   - Captures business seasonality\n",
    "   - Important for gambling: March (Cheltenham), December (Christmas)\n",
    "\n",
    "5. **Rolling Standard Deviation**\n",
    "   - Indicates volatility periods\n",
    "   - Higher std suggests more uncertainty in predictions\n",
    "\n",
    "### Lower-Impact (But Useful) Features\n",
    "\n",
    "6. **is_weekend, is_month_start, is_month_end**\n",
    "   - Binary flags for special days\n",
    "   - May capture payday effects\n",
    "\n",
    "7. **days_to_payday**\n",
    "   - Domain-specific feature\n",
    "   - May correlate with deposit spikes around 15th and month-end\n",
    "\n",
    "### Feature Engineering Recommendations\n",
    "\n",
    "- **For tree-based models** (XGBoost, LightGBM): Use all features including cyclical\n",
    "- **For linear models**: May need polynomial features or one-hot encoding for categorical\n",
    "- **For Prophet**: Uses built-in seasonality; external regressors optional\n",
    "- **Handle NaN values**: Drop initial rows or use forward fill for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for feature groups\n",
    "print(\"Feature Groups Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate average absolute correlation with targets\n",
    "feature_groups = {\n",
    "    'Temporal': temporal_features,\n",
    "    'Lag': lag_features,\n",
    "    'Rolling': rolling_features\n",
    "}\n",
    "\n",
    "for group_name, features in feature_groups.items():\n",
    "    if not features:\n",
    "        continue\n",
    "    \n",
    "    # Filter features that exist in corr_with_targets\n",
    "    valid_features = [f for f in features if f in corr_with_targets.index]\n",
    "    \n",
    "    if valid_features:\n",
    "        avg_corr_logins = corr_with_targets.loc[valid_features, 'daily_logins'].abs().mean()\n",
    "        avg_corr_deposits = corr_with_targets.loc[valid_features, 'daily_deposits'].abs().mean()\n",
    "        \n",
    "        print(f\"\\n{group_name} Features ({len(valid_features)} features):\")\n",
    "        print(f\"  Avg |corr| with daily_logins:   {avg_corr_logins:.3f}\")\n",
    "        print(f\"  Avg |corr| with daily_deposits: {avg_corr_deposits:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature-engineered data for model training\n",
    "output_path = project_root / \"data\" / \"processed\" / \"features.csv\"\n",
    "\n",
    "# Drop rows with NaN values (from lag/rolling features)\n",
    "df_clean = df_features.dropna()\n",
    "\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"Saved feature-engineered data to: {output_path}\")\n",
    "print(f\"Rows after removing NaN: {len(df_clean)} (dropped {len(df_features) - len(df_clean)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. **Loaded** synthetic volume data (2 years of daily login/deposit data)\n",
    "2. **Applied** the `FeaturePipeline` to generate 38+ features\n",
    "3. **Explored** feature categories: temporal, lag, and rolling\n",
    "4. **Visualized** correlations, lag relationships, rolling statistics, and cyclical patterns\n",
    "5. **Discussed** feature importance for different model types\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Lag features** (especially lag_7) are the strongest predictors due to weekly patterns\n",
    "- **Rolling features** help capture trends and smooth noise\n",
    "- **Cyclical encoding** properly represents periodic features for ML models\n",
    "- **Domain-specific features** (payday, events) add business context\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Train baseline models using these features\n",
    "- Compare feature importance across different model types\n",
    "- Consider additional features: external events, holidays, weather\n",
    "- Evaluate forecast accuracy at different horizons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
