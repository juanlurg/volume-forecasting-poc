{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Baseline Models Evaluation\n",
    "\n",
    "This notebook evaluates baseline forecasting models for volume prediction. Baseline models serve as essential benchmarks - any sophisticated model should outperform these simple approaches to justify its complexity.\n",
    "\n",
    "## Baseline Models Covered\n",
    "\n",
    "1. **Naive Model**: Predicts the last observed value for all future periods\n",
    "2. **Seasonal Naive Model**: Repeats the last seasonal pattern (weekly by default)\n",
    "3. **Moving Average Model**: Predicts using the average of the last N observations\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- **MAE** (Mean Absolute Error): Average absolute difference between predicted and actual values\n",
    "- **RMSE** (Root Mean Square Error): Penalizes larger errors more heavily than MAE\n",
    "- **MAPE** (Mean Absolute Percentage Error): Error as a percentage of actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Import baseline models\n",
    "from volume_forecast.models import NaiveModel, SeasonalNaiveModel, MovingAverageModel\n",
    "\n",
    "# Import evaluation tools\n",
    "from volume_forecast.evaluation import WalkForwardValidator, mae, rmse, mape\n",
    "\n",
    "# Import data generator (in case we need to create data)\n",
    "from volume_forecast.data_generation import VolumeGenerator\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We load the synthetic volume data generated in notebook 01. If the file does not exist, we generate it using the `VolumeGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "data_path = project_root / \"data\" / \"raw\" / \"synthetic_volumes.csv\"\n",
    "\n",
    "# Load or generate data\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "    print(f\"Loaded data from {data_path}\")\n",
    "else:\n",
    "    print(f\"Data file not found at {data_path}. Generating synthetic data...\")\n",
    "    generator = VolumeGenerator(seed=42)\n",
    "    df = generator.generate(\n",
    "        start_date=date(2023, 1, 1),\n",
    "        end_date=date(2024, 12, 31),\n",
    "        include_events=True\n",
    "    )\n",
    "    # Save for future use\n",
    "    data_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    generator.save(df, data_path)\n",
    "    print(f\"Generated and saved data to {data_path}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Total days: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split\n",
    "\n",
    "We split the data into training and validation sets:\n",
    "- **Training**: First 365 days (captures full yearly seasonality)\n",
    "- **Validation**: Remaining days (used for model evaluation)\n",
    "\n",
    "This split respects the temporal nature of the data - we never use future data to predict the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date to ensure chronological order\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Define split point\n",
    "INITIAL_TRAIN_DAYS = 365\n",
    "FORECAST_HORIZON = 7\n",
    "\n",
    "# Split data\n",
    "train_df = df.iloc[:INITIAL_TRAIN_DAYS].copy()\n",
    "valid_df = df.iloc[INITIAL_TRAIN_DAYS:].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df)} days ({train_df['date'].min()} to {train_df['date'].max()})\")\n",
    "print(f\"Validation set: {len(valid_df)} days ({valid_df['date'].min()} to {valid_df['date'].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the split\n",
    "TARGET = 'daily_logins'  # Primary target for forecasting\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(train_df['date'], train_df[TARGET], label='Training', color='steelblue', alpha=0.8)\n",
    "ax.plot(valid_df['date'], valid_df[TARGET], label='Validation', color='coral', alpha=0.8)\n",
    "\n",
    "# Add vertical line at split point\n",
    "ax.axvline(x=train_df['date'].iloc[-1], color='gray', linestyle='--', linewidth=2, label='Split Point')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Daily Logins')\n",
    "ax.set_title('Train/Validation Split')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model Evaluation\n",
    "\n",
    "We evaluate each baseline model by:\n",
    "1. Fitting on the training data\n",
    "2. Making a 7-day forecast\n",
    "3. Calculating evaluation metrics\n",
    "4. Visualizing actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for evaluation\n",
    "def evaluate_model(model, train_data, test_data, target, horizon=7):\n",
    "    \"\"\"\n",
    "    Fit model and calculate metrics on test data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with model name, metrics, and predictions\n",
    "    \"\"\"\n",
    "    # Fit model\n",
    "    model.fit(train_data, target)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(horizon=horizon)\n",
    "    \n",
    "    # Get actual values (first 'horizon' days of test set)\n",
    "    actuals = test_data[target].iloc[:horizon].values\n",
    "    preds = predictions['prediction'].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model._name,\n",
    "        'MAE': mae(actuals, preds),\n",
    "        'RMSE': rmse(actuals, preds),\n",
    "        'MAPE': mape(actuals, preds),\n",
    "    }\n",
    "    \n",
    "    # Store predictions with dates and actuals for plotting\n",
    "    result = {\n",
    "        'metrics': metrics,\n",
    "        'dates': test_data['date'].iloc[:horizon].values,\n",
    "        'actuals': actuals,\n",
    "        'predictions': preds\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Naive Model\n",
    "\n",
    "The simplest baseline: predict the last observed value for all future periods.\n",
    "\n",
    "**When it works well**: When the time series is a random walk with no trend or seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and evaluate Naive model\n",
    "naive_model = NaiveModel(name='naive')\n",
    "naive_results = evaluate_model(naive_model, train_df, valid_df, TARGET, FORECAST_HORIZON)\n",
    "\n",
    "print(\"Naive Model Results:\")\n",
    "print(f\"  MAE:  {naive_results['metrics']['MAE']:.2f}\")\n",
    "print(f\"  RMSE: {naive_results['metrics']['RMSE']:.2f}\")\n",
    "print(f\"  MAPE: {naive_results['metrics']['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Naive model predictions\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot last 14 days of training + 7 days forecast\n",
    "context_days = 14\n",
    "ax.plot(train_df['date'].iloc[-context_days:], train_df[TARGET].iloc[-context_days:], \n",
    "        'o-', color='steelblue', label='Training (last 14 days)', alpha=0.7)\n",
    "ax.plot(naive_results['dates'], naive_results['actuals'], \n",
    "        'o-', color='green', label='Actual', linewidth=2)\n",
    "ax.plot(naive_results['dates'], naive_results['predictions'], \n",
    "        's--', color='red', label='Naive Prediction', linewidth=2, markersize=8)\n",
    "\n",
    "ax.axvline(x=train_df['date'].iloc[-1], color='gray', linestyle=':', alpha=0.7)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Daily Logins')\n",
    "ax.set_title('Naive Model: Actual vs Predicted')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Seasonal Naive Model\n",
    "\n",
    "Predicts by repeating the last seasonal pattern. With `season_length=7`, it repeats the last week's values.\n",
    "\n",
    "**When it works well**: When the time series exhibits strong weekly seasonality (very common in business data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and evaluate Seasonal Naive model\n",
    "seasonal_naive = SeasonalNaiveModel(season_length=7, name='seasonal_naive')\n",
    "seasonal_results = evaluate_model(seasonal_naive, train_df, valid_df, TARGET, FORECAST_HORIZON)\n",
    "\n",
    "print(\"Seasonal Naive Model Results:\")\n",
    "print(f\"  MAE:  {seasonal_results['metrics']['MAE']:.2f}\")\n",
    "print(f\"  RMSE: {seasonal_results['metrics']['RMSE']:.2f}\")\n",
    "print(f\"  MAPE: {seasonal_results['metrics']['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Seasonal Naive model predictions\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(train_df['date'].iloc[-context_days:], train_df[TARGET].iloc[-context_days:], \n",
    "        'o-', color='steelblue', label='Training (last 14 days)', alpha=0.7)\n",
    "ax.plot(seasonal_results['dates'], seasonal_results['actuals'], \n",
    "        'o-', color='green', label='Actual', linewidth=2)\n",
    "ax.plot(seasonal_results['dates'], seasonal_results['predictions'], \n",
    "        's--', color='orange', label='Seasonal Naive Prediction', linewidth=2, markersize=8)\n",
    "\n",
    "ax.axvline(x=train_df['date'].iloc[-1], color='gray', linestyle=':', alpha=0.7)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Daily Logins')\n",
    "ax.set_title('Seasonal Naive Model: Actual vs Predicted')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Moving Average Model\n",
    "\n",
    "Predicts using the average of the last N observations. With `window=7`, it uses the average of the last week.\n",
    "\n",
    "**When it works well**: When the time series fluctuates around a stable mean with no strong seasonality or trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and evaluate Moving Average model\n",
    "ma_model = MovingAverageModel(window=7, name='moving_average_7')\n",
    "ma_results = evaluate_model(ma_model, train_df, valid_df, TARGET, FORECAST_HORIZON)\n",
    "\n",
    "print(\"Moving Average (7-day) Model Results:\")\n",
    "print(f\"  MAE:  {ma_results['metrics']['MAE']:.2f}\")\n",
    "print(f\"  RMSE: {ma_results['metrics']['RMSE']:.2f}\")\n",
    "print(f\"  MAPE: {ma_results['metrics']['MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Moving Average model predictions\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(train_df['date'].iloc[-context_days:], train_df[TARGET].iloc[-context_days:], \n",
    "        'o-', color='steelblue', label='Training (last 14 days)', alpha=0.7)\n",
    "ax.plot(ma_results['dates'], ma_results['actuals'], \n",
    "        'o-', color='green', label='Actual', linewidth=2)\n",
    "ax.plot(ma_results['dates'], ma_results['predictions'], \n",
    "        's--', color='purple', label='Moving Average Prediction', linewidth=2, markersize=8)\n",
    "\n",
    "ax.axvline(x=train_df['date'].iloc[-1], color='gray', linestyle=':', alpha=0.7)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Daily Logins')\n",
    "ax.set_title('Moving Average Model (7-day): Actual vs Predicted')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined Visualization\n",
    "\n",
    "Let's compare all three models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Training context\n",
    "ax.plot(train_df['date'].iloc[-context_days:], train_df[TARGET].iloc[-context_days:], \n",
    "        'o-', color='steelblue', label='Training', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Actual values\n",
    "ax.plot(naive_results['dates'], naive_results['actuals'], \n",
    "        'o-', color='green', label='Actual', linewidth=2.5, markersize=8)\n",
    "\n",
    "# Predictions from each model\n",
    "ax.plot(naive_results['dates'], naive_results['predictions'], \n",
    "        's--', color='red', label='Naive', linewidth=2, alpha=0.8)\n",
    "ax.plot(seasonal_results['dates'], seasonal_results['predictions'], \n",
    "        '^--', color='orange', label='Seasonal Naive', linewidth=2, alpha=0.8)\n",
    "ax.plot(ma_results['dates'], ma_results['predictions'], \n",
    "        'd--', color='purple', label='Moving Average', linewidth=2, alpha=0.8)\n",
    "\n",
    "ax.axvline(x=train_df['date'].iloc[-1], color='gray', linestyle=':', alpha=0.7, label='Train/Test Split')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Daily Logins')\n",
    "ax.set_title('Baseline Models Comparison: 7-Day Forecast')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Summary\n",
    "\n",
    "Let's create a summary table comparing all baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    naive_results['metrics'],\n",
    "    seasonal_results['metrics'],\n",
    "    ma_results['metrics']\n",
    "]\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(all_results)\n",
    "summary_df = summary_df.set_index('Model')\n",
    "\n",
    "# Add ranking columns\n",
    "summary_df['MAE_Rank'] = summary_df['MAE'].rank().astype(int)\n",
    "summary_df['RMSE_Rank'] = summary_df['RMSE'].rank().astype(int)\n",
    "summary_df['MAPE_Rank'] = summary_df['MAPE'].rank().astype(int)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODELS COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTarget: {TARGET}\")\n",
    "print(f\"Forecast Horizon: {FORECAST_HORIZON} days\")\n",
    "print(f\"Training Period: {train_df['date'].min().date()} to {train_df['date'].max().date()}\")\n",
    "print(f\"Test Period: {valid_df['date'].iloc[:FORECAST_HORIZON].min().date()} to {valid_df['date'].iloc[:FORECAST_HORIZON-1].max().date()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'MAPE']\n",
    "colors = ['steelblue', 'coral', 'seagreen']\n",
    "\n",
    "for ax, metric, color in zip(axes, metrics, colors):\n",
    "    bars = ax.bar(summary_df.index, summary_df[metric], color=color, alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel(f'{metric}{\" (%)\" if metric == \"MAPE\" else \"\"}')\n",
    "    ax.set_title(f'{metric} by Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, summary_df[metric]):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{val:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.suptitle('Baseline Models: Metric Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Baseline Selection\n",
    "\n",
    "Let's identify the best baseline model based on average ranking across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rank\n",
    "rank_cols = ['MAE_Rank', 'RMSE_Rank', 'MAPE_Rank']\n",
    "summary_df['Avg_Rank'] = summary_df[rank_cols].mean(axis=1)\n",
    "\n",
    "# Find best model\n",
    "best_model = summary_df['Avg_Rank'].idxmin()\n",
    "best_metrics = summary_df.loc[best_model]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST BASELINE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWinner: {best_model}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  - MAE:  {best_metrics['MAE']:.2f} (Rank: {int(best_metrics['MAE_Rank'])})\")\n",
    "print(f\"  - RMSE: {best_metrics['RMSE']:.2f} (Rank: {int(best_metrics['RMSE_Rank'])})\")\n",
    "print(f\"  - MAPE: {best_metrics['MAPE']:.2f}% (Rank: {int(best_metrics['MAPE_Rank'])})\")\n",
    "print(f\"  - Average Rank: {best_metrics['Avg_Rank']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "final_summary = summary_df[['MAE', 'RMSE', 'MAPE', 'Avg_Rank']].copy()\n",
    "final_summary = final_summary.sort_values('Avg_Rank')\n",
    "\n",
    "# Style the DataFrame\n",
    "def highlight_best(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['font-weight: bold' if v else '' for v in is_min]\n",
    "\n",
    "styled_summary = final_summary.style.apply(highlight_best)\n",
    "styled_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on our evaluation of the three baseline models:\n",
    "\n",
    "1. **Seasonal Naive typically performs best** for data with strong weekly patterns (like daily login/deposit volumes). This is because:\n",
    "   - It captures the day-of-week effects (e.g., higher volumes on weekends)\n",
    "   - Business metrics often have predictable weekly cycles\n",
    "\n",
    "2. **Moving Average smooths out noise** but loses seasonality information. It works better when:\n",
    "   - The series fluctuates around a stable mean\n",
    "   - There's high day-to-day variance but weak weekly patterns\n",
    "\n",
    "3. **Naive model is the simplest** and serves as the \"floor\" benchmark. If a sophisticated model cannot beat the Naive baseline, it's not adding value.\n",
    "\n",
    "### Implications for Model Development\n",
    "\n",
    "- Any ML/statistical model should **beat the best baseline** to justify its complexity\n",
    "- The MAPE values give us an error percentage target to beat\n",
    "- Seasonal patterns in the data suggest models that can capture seasonality (Prophet, SARIMA, features-based ML) should perform well\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Implement walk-forward validation across multiple test windows for more robust comparison\n",
    "2. Test statistical models (ARIMA, Prophet) against these baselines\n",
    "3. Build ML models with temporal features and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for use in other notebooks\n",
    "results_path = project_root / \"data\" / \"processed\" / \"baseline_results.csv\"\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "final_summary.to_csv(results_path)\n",
    "print(f\"Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 04 - Baseline Models Evaluation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
